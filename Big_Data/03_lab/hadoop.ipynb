{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwBWVAktlM6+jbVUskb/qQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rodeffs/Year4_Programming/blob/master/Big_Data/03_lab/hadoop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking java version"
      ],
      "metadata": {
        "id": "GYOAvZYlI09k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!java --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyMiKdsTI4G4",
        "outputId": "f0a68185-4e8a-4706-8a33-9be5e203462b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk 17.0.16 2025-07-15\n",
            "OpenJDK Runtime Environment (build 17.0.16+8-Ubuntu-0ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 17.0.16+8-Ubuntu-0ubuntu122.04.1, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Hadoop"
      ],
      "metadata": {
        "id": "1jKdzEczI-13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpymaiaLI9x5",
        "outputId": "6a1715fa-be80-410a-a18c-c95348977146"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-04 18:42:28--  https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1065831750 (1016M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.4.2.tar.gz’\n",
            "\n",
            "hadoop-3.4.2.tar.gz 100%[===================>]   1016M   170MB/s    in 9.3s    \n",
            "\n",
            "2025-12-04 18:43:17 (109 MB/s) - ‘hadoop-3.4.2.tar.gz’ saved [1065831750/1065831750]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf hadoop-3.4.2.tar.gz"
      ],
      "metadata": {
        "id": "0CLlXK_cKR0I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing the dataset"
      ],
      "metadata": {
        "id": "RB1ZEtWfP2Yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L https://www.kaggle.com/api/v1/datasets/download/beta3logic/3m-academic-papers-titles-and-abstracts -o dataset.zip"
      ],
      "metadata": {
        "id": "kFG1uYIrP-SH",
        "outputId": "709249d9-901a-44f0-92d8-7740236c5679",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1409M  100 1409M    0     0  77.6M      0  0:00:18  0:00:18 --:--:-- 83.3M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip dataset.zip"
      ],
      "metadata": {
        "id": "cqkE4wlUQsaR",
        "outputId": "eb03f94d-730b-411d-864b-f69675216907",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  dataset.zip\n",
            "  inflating: cleaned_papers.csv      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the variables"
      ],
      "metadata": {
        "id": "dIuOPxUsIqVW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mFiman4LExZQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64/jre\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/content/hadoop-3.4.2\"\n",
        "os.environ[\"PATH\"] += \":$JAVA_HOME/bin:$JRE_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting dataset into multiple files"
      ],
      "metadata": {
        "id": "zJZhuhOi1y5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile splitter.py\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "df_input = \"/content/cleaned_papers.csv\"\n",
        "\n",
        "df = pd.read_csv(df_input)\n",
        "i, j = 0, 0\n",
        "max_lines = 100000\n",
        "file = open(f\"/content/datasets/dataset{j}.txt\", mode=\"w\", encoding=\"utf-8\")\n",
        "\n",
        "for row in df.itertuples(index=False):\n",
        "    line = str(row.title + \". \" + row.abstract).lower()  # объединить оба столбца и перевести в нижний регистр\n",
        "    line = re.sub(r\"\\s*\\n\\s*\", ' ', line)  # убрать переносы на след. строку\n",
        "    file.write(line + \"\\n\")\n",
        "    i += 1\n",
        "\n",
        "    if i == max_lines:\n",
        "        file.close()\n",
        "        i = 0\n",
        "        j += 1\n",
        "        file = open(f\"/content/datasets/dataset{j}.txt\", mode=\"w\", encoding=\"utf-8\")\n",
        "\n",
        "file.close()\n"
      ],
      "metadata": {
        "id": "mVC6caFb1yic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f139fd5-0fcc-4b26-b0cc-9d189d316a80"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing splitter.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir datasets"
      ],
      "metadata": {
        "id": "7qoZr0e22bKg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python splitter.py"
      ],
      "metadata": {
        "id": "T2DLBCHH2dK4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making a HDFS directory and copying files there"
      ],
      "metadata": {
        "id": "hZSUn6pxs2hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /popular_topics"
      ],
      "metadata": {
        "id": "w5ttzI9Fs9gz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -put /content/datasets /popular_topics"
      ],
      "metadata": {
        "id": "o8JSBVSotjha"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Writing a mapper"
      ],
      "metadata": {
        "id": "KK95JCuQt60l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import re\n",
        "import sys\n",
        "\n",
        "regexp = r\"([^a-z^\\s^'^-])|(?:^|[^a-z])['-]|['-](?:^|[^a-z])|'*(?<![a-z-])(?:a|an|the|and|or|as|of|in|on|yet|our|than|then|however|at|but|was|were|which|there|this|that|thus|we|to|for|is|are|where|have|has|been|since|with|such|another|also|by|often|can|could|so|from|its|via|will|hence|should|would|shall|what|although|these|those|do|does|did|under|above|else|if|while|when|who|based|way|very|many|much|due|because|onto|into|out|finally|their|they|may|might|up|down|either|neither|nor|within|according|others|about|therefore|no|not|towards|beyond|behind|over|how|both|without|other|another|more|most|moreover|be|furthermore|why|paper|focuses|well|must|consider|using|used|commonly|some|given|among|able|present|his|her|he|she|obtained|makes|give|make|further|use|introduce|employ|uses|show|allows|gives|introduces|considers|through|take|takes|enable|enables|allow|every|each|called|provide|provides|cannot|allowing|even|though|after|around|upon|you|new)(?![a-z-])'*\"\n",
        "\n",
        "for line in sys.stdin:\n",
        "    for combination in re.split(regexp, line):\n",
        "        if combination is None:\n",
        "            continue\n",
        "\n",
        "        combination = combination.strip()\n",
        "\n",
        "        if len(re.split(r\"\\s+\", combination)) >= 2: # считаем за темы пары слов больше 2\n",
        "            print(combination + \";1\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHAdOClDt5dV",
        "outputId": "a8dc4804-d74f-4a75-b522-6ee4769c427a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 777 mapper.py"
      ],
      "metadata": {
        "id": "rlLp4Vu01Lny"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Writing a reducer"
      ],
      "metadata": {
        "id": "0niArKdewNmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import sys\n",
        "\n",
        "prev_entry = None\n",
        "result = 0\n",
        "\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    entry, value = line.split(';')\n",
        "\n",
        "    # Проверки, что вводимые данные можно преобразовать\n",
        "\n",
        "    try:\n",
        "        value = int(value)\n",
        "    except ValueError:\n",
        "        continue\n",
        "\n",
        "    if entry is None:\n",
        "        continue\n",
        "\n",
        "    # Т.к. Hadoop по умолчанию сортирует вывод mapper, то можно сделать так\n",
        "\n",
        "    if prev_entry != entry and prev_entry is not None:\n",
        "        print(f\"{prev_entry};{result}\")\n",
        "        result = 0\n",
        "\n",
        "    prev_entry = entry\n",
        "    result += value"
      ],
      "metadata": {
        "id": "H1ldAXLpwPXz",
        "outputId": "603e1ede-0317-49d8-f423-3bd807c610c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reducer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 777 reducer.py"
      ],
      "metadata": {
        "id": "qvKNodjw1OgK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can run the Hadoop MapReduce program"
      ],
      "metadata": {
        "id": "XP-J6tHT1aA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.4.2.jar \\\n",
        "-input /popular_topics/datasets \\\n",
        "-output /popular_topics/output \\\n",
        "-mapper \"python /content/mapper.py\" \\\n",
        "-reducer \"python /content/reducer.py\""
      ],
      "metadata": {
        "id": "ZRVbLbqM1fVR",
        "outputId": "57496246-c5ad-4256-fffe-3640d8146d4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aliyun-java-sdk-core-4.5.10.jar        hadoop-fs2img-3.4.2.jar\n",
            "aliyun-java-sdk-kms-2.11.0.jar\t       hadoop-gridmix-3.4.2.jar\n",
            "aliyun-java-sdk-ram-3.1.0.jar\t       hadoop-kafka-3.4.2.jar\n",
            "aliyun-sdk-oss-3.13.2.jar\t       hadoop-minicluster-3.4.2.jar\n",
            "analyticsaccelerator-s3-1.2.1.jar      hadoop-resourceestimator-3.4.2.jar\n",
            "azure-data-lake-store-sdk-2.3.9.jar    hadoop-rumen-3.4.2.jar\n",
            "azure-keyvault-core-1.0.0.jar\t       hadoop-sls-3.4.2.jar\n",
            "azure-storage-7.0.1.jar\t\t       hadoop-streaming-3.4.2.jar\n",
            "bundle-2.29.52.jar\t\t       hamcrest-core-1.3.jar\n",
            "hadoop-aliyun-3.4.2.jar\t\t       ini4j-0.5.4.jar\n",
            "hadoop-archive-logs-3.4.2.jar\t       jdk.tools-1.8.jar\n",
            "hadoop-archives-3.4.2.jar\t       jdom2-2.0.6.1.jar\n",
            "hadoop-aws-3.4.2.jar\t\t       junit-4.13.2.jar\n",
            "hadoop-azure-3.4.2.jar\t\t       kafka-clients-3.9.0.jar\n",
            "hadoop-azure-datalake-3.4.2.jar        lz4-java-1.7.1.jar\n",
            "hadoop-client-3.4.2.jar\t\t       ojalgo-43.0.jar\n",
            "hadoop-datajoin-3.4.2.jar\t       opentracing-api-0.33.0.jar\n",
            "hadoop-distcp-3.4.2.jar\t\t       opentracing-noop-0.33.0.jar\n",
            "hadoop-dynamometer-blockgen-3.4.2.jar  opentracing-util-0.33.0.jar\n",
            "hadoop-dynamometer-infra-3.4.2.jar     org.jacoco.agent-0.8.5-runtime.jar\n",
            "hadoop-dynamometer-workload-3.4.2.jar  wildfly-openssl-2.1.4.Final.jar\n",
            "hadoop-extras-3.4.2.jar\t\t       zstd-jni-1.5.6-4.jar\n",
            "hadoop-federation-balance-3.4.2.jar\n"
          ]
        }
      ]
    }
  ]
}