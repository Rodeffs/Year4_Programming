# Задания к лабораторным:

## Лабораторная работа 1:

Выбрать датасет размером не менее 1Gb (рекомендую использовать сайт kaggle.com), сформулировать вопрос по датасету, включающий в себя агрегацию данных и ранжирование результатов. Решить задача при помощи алгоритма MapReduce (без использования сторонних пактов его реализующих)

## Лабораторная работа 2:

1. Построить алгоритм произведения матриц через MapReduce
2. Построить алгоритм линейной регрессии через MapReduce
3. Продемонстрировать их работу

## Лабораторная работа 3:

1. Выбираем датасет (достаточно большой, желательно от 1Гб), формулируете вопрос, на который будете отвечать (можно работать с тем же датасетом, что и в первой лабораторной работе). Пытаемся ответить на него при помощи средств экосистемы hadoop: MapReducre Hadoop, YARN, HIVE, Impala, PIG, ....
2. Берем те же данные и тот же вопрос и отвечаем на него при помощи Apache Spark
3. Использовать витрину данных (Apache Airflow, Apache Nifi, AWS Step functions, Luigi, Apache Camel, ...) для формирования цепочки заданий для периодического запуска написанных алгоритмов и представления данных в удобном для пользователя виде

## Исследование технологии 1:

Нужно разбиться на команды по 2-3 человека (3 макс.) взять для исследования одну из тем:

- Решение для потокового ETL в Big Data (Apache Flume, Apache Nifi, ..)
- Sql для Hadoop (Apache Hive, Apache Phoenix, Apache Impala, ..)
- Scripting для Hadoop (Apache Pig, ...)
- ML для Hadoop (Mahout, ...)
- Scheduling Tools для Hadoop (Apache Oozie, Apache Azkaban, ...)
- Cluster Management Tools дл Hadoop (Apache Ambari:, Apache ZooKeeper:, ...)
- Инстурменты графовой аналитики для Spark (GraphX, GraphFrames, ..)
- ML для Spark (MLlib, ...)
- Интеграция Hadoop с Python
- Интеграция Spark  с Python

## Лабораторная работа 4:

Построить свой собственный мини-поисковик:

1. Выбрать некоторое число реальных сайтов или документов, которые имеют ссылки друг на друга и много текста
2. Реализовать парсинг документов по содержащимся словам и ссылкам, встречающимся в документах
3. Заполнить базу данных документов, слов и ссылок
4. Реализовать алгоритм PageRank для указанных документов при помощи MapReduce
5. Реализовать алгоритм PageRank для указанных документов при помощи графовой библиотеки Pregel
6. Построить полнотекстовый поиск по документам используя подходу document-at-a-time и term-at-a-time
7. Продемонстрировать работу всех выше указанных пунктов во взаимосвязи
