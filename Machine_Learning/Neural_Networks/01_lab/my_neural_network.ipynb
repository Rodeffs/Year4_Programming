{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32dc1b13-52b9-4f13-8ceb-c8da5b58d5c1",
   "metadata": {},
   "source": [
    "# Лабораторная работа №1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8d5c16-1faf-40b2-b214-abbb84dff942",
   "metadata": {},
   "source": [
    "## Задания\n",
    "\n",
    "Самостоятельно написать код, реализующий искусственный нейрон с сигма-функцией активации, и возможность строить на его основе многослойные сети. Код должен также реализовывать градиентный спуск и обратное распространение ошибки.\n",
    "\n",
    "На основе вашего кода:\n",
    "\n",
    "1. Решить задачу  классификации датаcета Iris одним нейроном.\n",
    "2. Решить задачу  классификации датаcета Iris одним  нейросетью из 2 слоев по 10 нейронов в слое.\n",
    "3. Отрисовать разделяющую линию для обеих моделей. Сравнить метрики классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e874744-dd80-49c2-8fb3-937ee39e9b75",
   "metadata": {},
   "source": [
    "## Реализация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced18f6c-63a0-4d64-98d5-d34949c427ff",
   "metadata": {},
   "source": [
    "### Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bc1fe0b-3c79-40b0-9929-51da56945f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c81ddf1-6093-4df9-b5ab-3c367e1549f4",
   "metadata": {},
   "source": [
    "### Функция активации\n",
    "\n",
    "В качестве неё берём сигма-функцию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ba2eb9a-5d71-42a1-9139-e3375a273932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a3d558-61c1-4f7a-bce3-1c10edaaec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def der_activation(x):  # производная функции активации (уже посчитан вывод)\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5fd2c8-0636-4309-b841-db0a29e32f14",
   "metadata": {},
   "source": [
    "### Функция ошибок\n",
    "\n",
    "Т.к. это задача классификации, то будет использоваться функция ошибок Categorical Cross-Entropy Loss\n",
    "\n",
    "Её формула:\n",
    "\n",
    "```\n",
    "-sum_i(sum_j(y_ij * log(z_ij) + (1 - y_ij) * log(1 - z_ij)))\n",
    "```\n",
    "\n",
    "где\n",
    "\n",
    "* 1 <= i <= N, N - кол-во входных данных\n",
    "* 1 <= j <= C, C - кол-во классов\n",
    "* y_ij - индикатор 0 или 1, что класс j является правильным для элемента i\n",
    "* z_ij - предсказанная вероятность того, что у элемента i класс j\n",
    "* log - логарифм\n",
    "* sum_i - сумма по i\n",
    "* sum_j - сумма по j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b50161bd-f875-47be-bf5a-a4e990686329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "    C = len(y_true[0])\n",
    "\n",
    "    total_sum = 0\n",
    "\n",
    "    for i in range(N):\n",
    "        value_sum = 0\n",
    "        \n",
    "        for j in range(C):\n",
    "            p = y_pred[i][j]\n",
    "            t = y_true[i][j]\n",
    "\n",
    "            if p == 0:\n",
    "                value_sum += t\n",
    "\n",
    "            elif p == 1:\n",
    "                value_sum += (1-t)\n",
    "\n",
    "            else:\n",
    "                value_sum += t * np.log(p) + (1 - t) * np.log(1 - p)\n",
    "            \n",
    "        total_sum += value_sum\n",
    "            \n",
    "    return -total_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29484cd8-0be7-4064-85bf-4f9be0e1bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def der_mult(y_true, y_pred):  # произведение производных функции ошибок и функции активации\n",
    "    return y_pred - y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9dd3c6-6466-4bd4-a646-e87b41e7c55d",
   "metadata": {},
   "source": [
    "### Слой нейронной сети\n",
    "\n",
    "Хранит матрицу весов нейронов, матрицу сдвигов нейронов, матрицу ошибок и вывод нейронов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef66cdba-e574-4124-be84-7c04deb8791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, neuron_count):\n",
    "        self.neuron_count = neuron_count\n",
    "        self.weights = None  # матрица весов нейронов N x W, где N - кол-во нейронов в слое, а W - кол-во входных данных в слой\n",
    "        self.biases = None  # вектор сдвигов нейронов N x 1, где N - кол-во нейронов в слое\n",
    "        self.output = None  # вектор вывода N x 1\n",
    "        self.error = None  # вектор ошибок N x 1, нужен для обратного распространения ошибки в градиентном спуске\n",
    "\n",
    "    \n",
    "    def setup_layer(self, input_count):\n",
    "        self.weights = np.zeros((self.neuron_count, input_count))\n",
    "\n",
    "        for i in range(self.neuron_count):  # изначальные веса равны N(0, 2 / (n + w)), где n - кол-во нейронов, w - кол-во входов, N - нормальное распределение\n",
    "            for j in range(input_count):\n",
    "                self.weights[i][j] = np.random.normal(0, 2 / (self.neuron_count + input_count))  \n",
    "                \n",
    "        self.biases = np.zeros((self.neuron_count, 1))  # изначальный сдвиг равен 0\n",
    "\n",
    "    \n",
    "    def input_output(self, input_values):\n",
    "        self.output = activation((self.weights @ input_values) + self.biases)  # вектор вывода, здесь F(WX + B), где X - вектор входных данных, W - матрица весов, B - вектор сдвигов\n",
    "        \n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ed3db-c3bb-48f9-b13f-b3caef263014",
   "metadata": {},
   "source": [
    "### Нейронная сеть\n",
    "\n",
    "Сеть состоит из слоёв, здесь определяется её поведение\n",
    "\n",
    "X и y нужно преобразовать перед тем, как работать с ними\n",
    "\n",
    "Предположим, для задачи классификации есть 4 возможных вывода (0, 1, 2, 3) и y выглядит так:\n",
    "\n",
    "```\n",
    "| y |\n",
    "|---|\n",
    "| 0 |\n",
    "| 2 |\n",
    "| 1 |\n",
    "| 2 | \n",
    "| 3 |\n",
    "| 1 |\n",
    "```\n",
    "\n",
    "Нейроны выдают значения от 0 до 1, и чтобы классификация прошла правильно, преобразуем данные в матрицу вхождений\n",
    "\n",
    "```\n",
    "| y |     |    y    |\n",
    "|---|     |---------|\n",
    "| 0 | --> | 1 0 0 0 |\n",
    "| 2 | --> | 0 0 1 0 |\n",
    "| 1 | --> | 0 1 0 0 |\n",
    "| 2 | --> | 0 0 1 0 |\n",
    "| 3 | --> | 0 0 0 1 |\n",
    "| 1 | --> | 0 1 0 0 | \n",
    "```\n",
    "\n",
    "Также нужно нормализировать входные данные в пределах (-1, 1), тогда mean = 0, а var = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d8896ba-a343-4b00-9d13-cc66327e0467",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNetwork:\n",
    "    def __init__(self, layers=[]):\n",
    "        self.layers = layers\n",
    "\n",
    "    \n",
    "    def back_propogation(self, true_values):  # обратное распространение ошибки\n",
    "        for layer in range(len(self.layers) - 1, -1, -1):  # идём в обратном порядке, в этом суть метода\n",
    "            cur_layer = self.layers[layer]\n",
    "            \n",
    "            if layer == len(self.layers) - 1:\n",
    "                # Для выходного слоя вектор ошибок будет выглядеть так: L'(t, o) * f'(i), где:\n",
    "                # L' - производная функции ошибок, f' - производная функции активации\n",
    "                # o - вывод значений нынешнего слоя, t - истинный вывод, i - вывод значений предыдущего слоя\n",
    "                # Важное замечание: у функции активации сигма, производная равна f(i)(1 - f(i))\n",
    "                # Т.к. мы уже знаем, что o = f(i), то тогда f'(i) = o(1 - o)\n",
    "                \n",
    "                cur_layer.error = der_mult(true_values, cur_layer.output)\n",
    "\n",
    "            else:\n",
    "                # Для остальных слоёв вектор ошибок будет такой: TD * f'(i), где:\n",
    "                # D - вектор ошибок следующего слоя, T - транспонированная матрица весов следующего слоя\n",
    "                # f' - производная функции активации, o - вывод значений нынешнего слоя, i - вывод значений предыдущего слоя\n",
    "                # Важное замечание: у функции активации сигма, производная равна f(i)(1 - f(i))\n",
    "                # Т.к. мы уже знаем, что o = f(i), то тогда f'(i) = o(1 - o)\n",
    "                \n",
    "                next_layer = self.layers[layer + 1]\n",
    "                cur_layer.error = (next_layer.weights.transpose() @ next_layer.error) * der_activation(cur_layer.output)\n",
    "                \n",
    "    \n",
    "    def gradient_descent(self, input_values, learning_rate):  # градиентный спуск\n",
    "        for layer in range(len(self.layers)):  # теперь идём по порядку, так называемый forward progogation     \n",
    "            cur_layer = self.layers[layer]\n",
    "\n",
    "            # Изменение сдвигов равно E*l, где l - скорость обучения, E - вектор ошибок\n",
    "\n",
    "            delta_biases = cur_layer.error * learning_rate \n",
    "\n",
    "            # Изменение весов равно ET*l, где l - скорость обучения, E - вектор ошибок, T - транспонированная матрица вводных данных слоя\n",
    "\n",
    "            delta_weights = None\n",
    "\n",
    "            if layer == 0:\n",
    "                delta_weights = (cur_layer.error @ input_values.transpose()) * learning_rate\n",
    "\n",
    "            else:\n",
    "                prev_layer = self.layers[layer - 1]\n",
    "                delta_weights = (cur_layer.error @ prev_layer.output.transpose()) * learning_rate\n",
    "\n",
    "            cur_layer.weights -= delta_weights\n",
    "            cur_layer.biases -= delta_biases\n",
    "\n",
    "\n",
    "    def normalise(self, X):  # нормализация входных данных\n",
    "        normalised = np.zeros(X.shape)\n",
    "    \n",
    "        for i in range(normalised.shape[1]):        \n",
    "            normalised[:, i] = (X[:, i] - X[:, i].mean()) / (X[:, i].max() - X[:, i].min())\n",
    "    \n",
    "        return normalised\n",
    "\n",
    "\n",
    "    def one_hot_transform(self, y, classes_count):  # преобразование в матрицу вхождений\n",
    "        transformed = np.zeros((len(y), classes_count))\n",
    "    \n",
    "        for i in range(len(y)):  # исходит из предположения, что классы будут пронумерованы по порядку, начиная от 0\n",
    "            transformed[i][y[i]] = 1\n",
    "    \n",
    "        return transformed\n",
    "\n",
    "        \n",
    "    def fit(self, X, y, learning_rate=0.1, epochs=1000, verbose=False, early_stopping=None):        \n",
    "        X_train = self.normalise(X)\n",
    "        y_train = self.one_hot_transform(y, self.layers[-1].neuron_count)  # столько классифицируем, сколько нейронов на последнем слое\n",
    "        \n",
    "        for layer in range(len(self.layers)):  # инициализация слоёв\n",
    "            if layer == 0:\n",
    "                self.layers[layer].setup_layer(len(X_train[0]))  # первый слой - входной, у него кол-во входных данных равно кол-ву вводимых атрибутов\n",
    "\n",
    "            else:\n",
    "                self.layers[layer].setup_layer(self.layers[layer-1].neuron_count)  # у каждого последующего слоя кол-во входных данных равно кол-ву нейронов на предыдущем слое\n",
    "\n",
    "        last_error = None\n",
    "\n",
    "        for epoch in range(epochs):  # начало обучения\n",
    "            y_pred = []\n",
    "            \n",
    "            for value in range(len(X_train)):\n",
    "                input_values = np.array([X_train[value]]).transpose()  # входные данные в виде вектор-столбца\n",
    "                true_values = np.array([y_train[value]]).transpose()  # истинные данные в виде вектор-столбца\n",
    "                \n",
    "                neuron_output = input_values  # в первый слой идут входные данные\n",
    "\n",
    "                for layer in range(len(self.layers)):       \n",
    "                    neuron_output = self.layers[layer].input_output(neuron_output)\n",
    "\n",
    "                y_pred.append(neuron_output.transpose()[0])  # значение последнего слоя и будет предсказанным значением\n",
    "                \n",
    "                self.back_propogation(true_values)  # считаем обратное распространение ошибки\n",
    "                self.gradient_descent(input_values, learning_rate)  # выполняем градиентный спуск\n",
    "\n",
    "            y_pred = np.array(y_pred)\n",
    "            error = loss(y_train, y_pred)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch+1} out of {epochs}, current error is {error}\")\n",
    "\n",
    "            if early_stopping is not None:  # early stopping - минимальное значение разницы ошибок последних двух итераций, меньше которого обучение останавливается\n",
    "                if last_error is not None:\n",
    "                    if np.abs(error - last_error) < early_stopping:\n",
    "                        if verbose:\n",
    "                            print(\"Stopping early\")\n",
    "                        return\n",
    "\n",
    "                last_error = error\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_test = self.normalise(X)\n",
    "        y_pred = []  # предсказанные значения\n",
    "\n",
    "        for value in range(len(X_test)):            \n",
    "            neuron_output = np.array([X_test[value]]).transpose()  # входные данные в виде вектор-столбца\n",
    "\n",
    "            for layer in range(len(self.layers)):    \n",
    "                neuron_output = self.layers[layer].input_output(neuron_output)\n",
    "\n",
    "            probab = neuron_output.transpose()[0] # значение последнего слоя и будет предсказанным значением - вероятности для каждого класса\n",
    "            y_pred.append(np.argmax(probab))  # выбираем класс с наибольшей вероятностью\n",
    "\n",
    "        return np.array(y_pred)  # получили предсказание"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c63b78b-0254-4f21-a51c-849b7f09e859",
   "metadata": {},
   "source": [
    "### Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d494681-d500-4fac-845a-61b4417d5cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_iris()\n",
    "X, y = df[\"data\"], df[\"target\"]\n",
    "classes_count = len(df[\"target_names\"])  # кол-во классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e969371d-90ce-4e61-8d59-d249ef9a6db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 1, 2, 1, 0, 1, 1, 0, 2, 0, 2, 1, 0, 1, 1, 0, 2, 1, 2, 2,\n",
       "       2, 2, 1, 1, 0, 0, 1, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb8bd4a-7478-4bfc-9aaf-a9560f0c77ec",
   "metadata": {},
   "source": [
    "### Сеть из 1 слоя с 1 нейроном"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "012537a3-f030-4598-bc35-5e8900a294c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = ClassificationNetwork([\n",
    "    Layer(1),\n",
    "    Layer(classes_count)  # кол-во нейронов на выходном слою равно кол-ву типов классификации\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "009d8419-3594-49e6-87eb-20d8ea6a50e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(X_train, y_train, verbose=False, learning_rate=0.1, epochs=5000, early_stopping=10**-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f5e7e0c-1382-4d83-9216-04eed6aa745e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 2, 0, 1, 1, 0, 2, 0, 2, 1, 0, 1, 1, 0, 2, 1, 2, 2,\n",
       "       2, 2, 1, 1, 0, 0, 2, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred1 = model1.predict(X_test)\n",
    "y_pred1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ec61da-805a-4f7c-9d63-4f767300ccc4",
   "metadata": {},
   "source": [
    "### Сеть из 2 слоёв по 10 нейронов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54272f86-c1bc-4635-bfce-7c51a82228b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ClassificationNetwork([\n",
    "    Layer(10),\n",
    "    Layer(10),\n",
    "    Layer(classes_count)  # кол-во нейронов на выходном слою равно кол-ву типов классификации\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df4c92c9-cff7-4f22-9bda-1aeb93572fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(X_train, y_train, verbose=False, learning_rate=0.1, epochs=5000, early_stopping=10**-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4aefca00-aeef-4b9d-bc2c-613873bab194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 2, 0, 2, 1, 0, 1, 1, 0, 2, 1, 2, 2,\n",
       "       2, 2, 1, 0, 0, 0, 2, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred2 = model2.predict(X_test)\n",
    "y_pred2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local",
   "language": "python",
   "name": "local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
